# æ€§èƒ½æµ‹è¯•ä¸è°ƒä¼˜

> **AIç”Ÿæˆå£°æ˜**: æœ¬æ–‡æ¡£ç”±AIè¾…åŠ©ç”Ÿæˆï¼Œæ—¨åœ¨æä¾›æ€§èƒ½æµ‹è¯•ä¸è°ƒä¼˜çš„åŸºç¡€çŸ¥è¯†å’Œå®è·µæŒ‡å—ã€‚

## ğŸ¯ å­¦ä¹ ç›®æ ‡

é€šè¿‡æœ¬ç« èŠ‚çš„å­¦ä¹ ,ä½ å°†èƒ½å¤Ÿ:

- ç†è§£æ€§èƒ½æµ‹è¯•çš„ç±»å‹å’Œæ–¹æ³•
- æŒæ¡æ€§èƒ½æµ‹è¯•å·¥å…·çš„ä½¿ç”¨
- äº†è§£æ€§èƒ½è°ƒä¼˜çš„æ­¥éª¤å’ŒæŠ€å·§
- å­¦ä¹ æ€§èƒ½æµ‹è¯•çš„å®è·µåº”ç”¨

## ğŸ“š æ€§èƒ½æµ‹è¯•ç±»å‹

### 1. è´Ÿè½½æµ‹è¯•(Load Testing)

åœ¨æ­£å¸¸å’Œé¢„æœŸå³°å€¼è´Ÿè½½æ¡ä»¶ä¸‹æµ‹è¯•ç³»ç»Ÿæ€§èƒ½ã€‚

**ç›®æ ‡**: éªŒè¯ç³»ç»Ÿåœ¨æ­£å¸¸è´Ÿè½½ä¸‹çš„æ€§èƒ½

```python
import requests
import time
from concurrent.futures import ThreadPoolExecutor
import statistics

class LoadTester:
    def __init__(self, url, concurrent_users=10, duration=60):
        self.url = url
        self.concurrent_users = concurrent_users
        self.duration = duration
        self.results = []
    
    def send_request(self):
        """å‘é€å•ä¸ªè¯·æ±‚"""
        start_time = time.time()
        try:
            response = requests.get(self.url, timeout=10)
            elapsed = (time.time() - start_time) * 1000
            return {
                'status_code': response.status_code,
                'response_time': elapsed,
                'success': response.status_code == 200
            }
        except Exception as e:
            return {
                'status_code': 0,
                'response_time': (time.time() - start_time) * 1000,
                'success': False,
                'error': str(e)
            }
    
    def run_test(self):
        """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
        end_time = time.time() + self.duration
        request_count = 0
        
        with ThreadPoolExecutor(max_workers=self.concurrent_users) as executor:
            while time.time() < end_time:
                futures = []
                for _ in range(self.concurrent_users):
                    future = executor.submit(self.send_request)
                    futures.append(future)
                
                for future in futures:
                    result = future.result()
                    self.results.append(result)
                    request_count += 1
        
        return self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        response_times = [r['response_time'] for r in self.results if r['success']]
        success_count = sum(1 for r in self.results if r['success'])
        total_count = len(self.results)
        
        if not response_times:
            return {'error': 'No successful requests'}
        
        return {
            'total_requests': total_count,
            'successful_requests': success_count,
            'failed_requests': total_count - success_count,
            'success_rate': success_count / total_count * 100,
            'avg_response_time': statistics.mean(response_times),
            'median_response_time': statistics.median(response_times),
            'p95_response_time': statistics.quantiles(response_times, n=20)[18],
            'p99_response_time': statistics.quantiles(response_times, n=100)[98],
            'min_response_time': min(response_times),
            'max_response_time': max(response_times),
            'throughput': success_count / self.duration  # RPS
        }

# ä½¿ç”¨ç¤ºä¾‹
tester = LoadTester('http://example.com/api', concurrent_users=50, duration=60)
report = tester.run_test()
print(report)
```

### 2. å‹åŠ›æµ‹è¯•(Stress Testing)

åœ¨è¶…å‡ºæ­£å¸¸è´Ÿè½½çš„æ¡ä»¶ä¸‹æµ‹è¯•ç³»ç»Ÿã€‚

**ç›®æ ‡**: æ‰¾åˆ°ç³»ç»Ÿçš„æ€§èƒ½æé™

### 3. å®¹é‡æµ‹è¯•(Capacity Testing)

æµ‹è¯•ç³»ç»Ÿåœ¨ç‰¹å®šè´Ÿè½½ä¸‹çš„å®¹é‡ã€‚

**ç›®æ ‡**: ç¡®å®šç³»ç»Ÿèƒ½å¤Ÿæ”¯æŒçš„æœ€å¤§ç”¨æˆ·æ•°æˆ–è´Ÿè½½

### 4. ç¨³å®šæ€§æµ‹è¯•(Stability Testing)

é•¿æ—¶é—´è¿è¡Œæµ‹è¯•,æ£€æŸ¥å†…å­˜æ³„æ¼ç­‰é—®é¢˜ã€‚

**ç›®æ ‡**: éªŒè¯ç³»ç»Ÿçš„ç¨³å®šæ€§

## ğŸ”§ æ€§èƒ½æµ‹è¯•å·¥å…·

### 1. Apache JMeter

```python
# JMeterå¯ä»¥é€šè¿‡Pythonè„šæœ¬è°ƒç”¨
import subprocess
import json

def run_jmeter_test(plan_file, result_file):
    """è¿è¡ŒJMeteræµ‹è¯•è®¡åˆ’"""
    cmd = [
        'jmeter',
        '-n',  # éGUIæ¨¡å¼
        '-t', plan_file,  # æµ‹è¯•è®¡åˆ’æ–‡ä»¶
        '-l', result_file  # ç»“æœæ–‡ä»¶
    ]
    
    result = subprocess.run(cmd, capture_output=True, text=True)
    return result.returncode == 0

def parse_jmeter_results(result_file):
    """è§£æJMeterç»“æœ"""
    # è§£æCSVç»“æœæ–‡ä»¶
    import csv
    
    results = []
    with open(result_file, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            results.append({
                'timestamp': row['timeStamp'],
                'elapsed': float(row['elapsed']),
                'success': row['success'] == 'true',
                'response_code': row['responseCode']
            })
    
    return results
```

### 2. Locust

```python
from locust import HttpUser, task, between

class WebsiteUser(HttpUser):
    wait_time = between(1, 3)  # ç”¨æˆ·ç­‰å¾…æ—¶é—´
    
    @task(3)  # æƒé‡3,æ‰§è¡Œé¢‘ç‡æ›´é«˜
    def index(self):
        self.client.get("/")
    
    @task(1)  # æƒé‡1
    def about(self):
        self.client.get("/about")
    
    def on_start(self):
        """ç”¨æˆ·å¯åŠ¨æ—¶æ‰§è¡Œ"""
        # ç™»å½•ç­‰æ“ä½œ
        pass

# è¿è¡Œ: locust -f locustfile.py
```

### 3. wrk

```bash
# åŸºæœ¬ç”¨æ³•
wrk -t12 -c400 -d30s http://example.com

# ä½¿ç”¨Luaè„šæœ¬
wrk -t12 -c400 -d30s -s script.lua http://example.com
```

## ğŸš€ æ€§èƒ½è°ƒä¼˜æµç¨‹

### 1. åŸºçº¿æµ‹è¯•

å»ºç«‹æ€§èƒ½åŸºçº¿,ä½œä¸ºå¯¹æ¯”åŸºå‡†ã€‚

```python
def baseline_test():
    """å»ºç«‹æ€§èƒ½åŸºçº¿"""
    metrics = {
        'response_time': [],
        'throughput': [],
        'error_rate': []
    }
    
    # è¿è¡Œæµ‹è¯•å¹¶æ”¶é›†æŒ‡æ ‡
    for _ in range(10):
        result = run_performance_test()
        metrics['response_time'].append(result['avg_response_time'])
        metrics['throughput'].append(result['throughput'])
        metrics['error_rate'].append(result['error_rate'])
    
    baseline = {
        'avg_response_time': statistics.mean(metrics['response_time']),
        'avg_throughput': statistics.mean(metrics['throughput']),
        'avg_error_rate': statistics.mean(metrics['error_rate'])
    }
    
    return baseline
```

### 2. ç“¶é¢ˆè¯†åˆ«

æ‰¾å‡ºæ€§èƒ½ç“¶é¢ˆã€‚

```python
import cProfile
import pstats
from io import StringIO

def identify_bottleneck(func):
    """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
    profiler = cProfile.Profile()
    profiler.enable()
    
    func()
    
    profiler.disable()
    s = StringIO()
    stats = pstats.Stats(profiler, stream=s)
    stats.sort_stats('cumulative')
    stats.print_stats(20)
    
    return s.getvalue()
```

### 3. ä¼˜åŒ–å®æ–½

æ ¹æ®ç“¶é¢ˆè¿›è¡Œä¼˜åŒ–ã€‚

### 4. éªŒè¯ä¼˜åŒ–

éªŒè¯ä¼˜åŒ–æ•ˆæœã€‚

```python
def validate_optimization(baseline, optimized):
    """éªŒè¯ä¼˜åŒ–æ•ˆæœ"""
    improvements = {
        'response_time_improvement': 
            (baseline['avg_response_time'] - optimized['avg_response_time']) / baseline['avg_response_time'] * 100,
        'throughput_improvement': 
            (optimized['avg_throughput'] - baseline['avg_throughput']) / baseline['avg_throughput'] * 100,
        'error_rate_improvement': 
            (baseline['avg_error_rate'] - optimized['avg_error_rate']) / baseline['avg_error_rate'] * 100
    }
    
    return improvements
```

## ğŸ“Š æ€§èƒ½ç›‘æ§

### å®æ—¶ç›‘æ§

```python
import psutil
import time
from collections import deque

class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            'cpu': deque(maxlen=100),
            'memory': deque(maxlen=100),
            'response_time': deque(maxlen=100)
        }
    
    def collect_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        while True:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            
            self.metrics['cpu'].append(cpu_percent)
            self.metrics['memory'].append(memory.percent)
            
            time.sleep(1)
    
    def get_statistics(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'avg_cpu': statistics.mean(self.metrics['cpu']),
            'max_cpu': max(self.metrics['cpu']),
            'avg_memory': statistics.mean(self.metrics['memory']),
            'max_memory': max(self.metrics['memory'])
        }
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. æµ‹è¯•ç¯å¢ƒ

- æµ‹è¯•ç¯å¢ƒè¦ä¸ç”Ÿäº§ç¯å¢ƒå°½å¯èƒ½ç›¸ä¼¼
- æ•°æ®é‡è¦è¶³å¤Ÿå¤§
- ç½‘ç»œæ¡ä»¶è¦ä¸€è‡´

### 2. æµ‹è¯•æ•°æ®

- ä½¿ç”¨çœŸå®çš„æµ‹è¯•æ•°æ®
- æ•°æ®åˆ†å¸ƒè¦ç¬¦åˆå®é™…æƒ…å†µ
- é¿å…ä½¿ç”¨ç¼“å­˜æ•°æ®

### 3. ç»“æœåˆ†æ

- å…³æ³¨P95ã€P99æŒ‡æ ‡
- åˆ†æé”™è¯¯ç±»å‹å’ŒåŸå› 
- å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ•ˆæœ

## ğŸ“– æ¨èèµ„æº

- [JMeterå®˜æ–¹æ–‡æ¡£](https://jmeter.apache.org/)
- [Locustå®˜æ–¹æ–‡æ¡£](https://docs.locust.io/)
- ã€Šæ€§èƒ½æµ‹è¯•å®æˆ˜ã€‹ä¹¦ç±

## ğŸ’¡ ä¸‹ä¸€æ­¥

- å­¦ä¹ [æ€§èƒ½ç›‘æ§](./performance-monitoring.md)
- äº†è§£[é«˜æ€§èƒ½æ¶æ„è®¾è®¡](./architecture-design.md)

---

*æœ€åæ›´æ–°æ—¶é—´: 2025-01-20*

